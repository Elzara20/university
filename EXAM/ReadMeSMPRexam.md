# Экзамен СМПР

[1. Задача обучения по прецедентам. Основные понятия и определения](https://github.com/Elzara20/university/blob/master/EXAM/ReadMeSMPRexam.md#1%D0%B7%D0%B0%D0%B4%D0%B0%D1%87%D0%B0-%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D1%8F-%D0%BF%D0%BE-%D0%BF%D1%80%D0%B5%D1%86%D0%B5%D0%B4%D0%B5%D0%BD%D1%82%D0%B0%D0%BC-%D0%BE%D1%81%D0%BD%D0%BE%D0%B2%D0%BD%D1%8B%D0%B5-%D0%BF%D0%BE%D0%BD%D1%8F%D1%82%D0%B8%D1%8F-%D0%B8-%D0%BE%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D1%8F)

[2. Модель алгоритмов и метод обучения. Функционал качества и функция потерь](https://github.com/Elzara20/university/blob/master/EXAM/ReadMeSMPRexam.md#2-%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C-%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D0%BE%D0%B2-%D0%B8-%D0%BC%D0%B5%D1%82%D0%BE%D0%B4-%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D1%8F-%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D0%BE%D0%BD%D0%B0%D0%BB-%D0%BA%D0%B0%D1%87%D0%B5%D1%81%D1%82%D0%B2%D0%B0-%D0%B8-%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F-%D0%BF%D0%BE%D1%82%D0%B5%D1%80%D1%8C)

[3. Проблема переобучения и понятие обобщающей способности. Эмпирические оценки обобщающей способности. Разновидности скользящего контроля](https://github.com/Elzara20/university/blob/master/EXAM/ReadMeSMPRexam.md#3-%D0%BF%D1%80%D0%BE%D0%B1%D0%BB%D0%B5%D0%BC%D0%B0-%D0%BF%D0%B5%D1%80%D0%B5%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D1%8F-%D0%B8-%D0%BF%D0%BE%D0%BD%D1%8F%D1%82%D0%B8%D0%B5-%D0%BE%D0%B1%D0%BE%D0%B1%D1%89%D0%B0%D1%8E%D1%89%D0%B5%D0%B9-%D1%81%D0%BF%D0%BE%D1%81%D0%BE%D0%B1%D0%BD%D0%BE%D1%81%D1%82%D0%B8-%D1%8D%D0%BC%D0%BF%D0%B8%D1%80%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B5-%D0%BE%D1%86%D0%B5%D0%BD%D0%BA%D0%B8-%D0%BE%D0%B1%D0%BE%D0%B1%D1%89%D0%B0%D1%8E%D1%89%D0%B5%D0%B9-%D1%81%D0%BF%D0%BE%D1%81%D0%BE%D0%B1%D0%BD%D0%BE%D1%81%D1%82%D0%B8-%D1%80%D0%B0%D0%B7%D0%BD%D0%BE%D0%B2%D0%B8%D0%B4%D0%BD%D0%BE%D1%81%D1%82%D0%B8-%D1%81%D0%BA%D0%BE%D0%BB%D1%8C%D0%B7%D1%8F%D1%89%D0%B5%D0%B3%D0%BE-%D0%BA%D0%BE%D0%BD%D1%82%D1%80%D0%BE%D0%BB%D1%8F)

[4-6 Метрические алгоритмы классификации](https://github.com/Elzara20/university/blob/master/EXAM/ReadMeSMPRexam.md#4-6-%D0%BC%D0%B5%D1%82%D1%80%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B5-%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D1%8B-%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D0%B8)


[4. Метрические алгоритмы классификации: алгоритмы ближайших соседей](https://github.com/Elzara20/university/blob/master/EXAM/ReadMeSMPRexam.md#4-%D0%BC%D0%B5%D1%82%D1%80%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B5-%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D1%8B-%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D0%B8-%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D1%8B-%D0%B1%D0%BB%D0%B8%D0%B6%D0%B0%D0%B9%D1%88%D0%B8%D1%85-%D1%81%D0%BE%D1%81%D0%B5%D0%B4%D0%B5%D0%B9)


## 1.Задача обучения по прецедентам. Основные понятия и определения.
Задано множество объектов ![](https://latex.codecogs.com/gif.latex?X), множество допустимых ответов ![](https://latex.codecogs.com/gif.latex?Y), и существует целевая функция (target function) ![](https://latex.codecogs.com/gif.latex?y^*:X&space;\rightarrow&space;Y) значения которой ![](https://latex.codecogs.com/gif.latex?y_i=y^*(x_i)) известны только на конечном подмножестве объектов ![](https://latex.codecogs.com/gif.latex?x_i&space;\subset&space;X). Пары «объект–ответ» (xi, yi) называются прецедентами. Совокупность пар ![](https://latex.codecogs.com/gif.latex?X^l=(x_i,y_i)_{i=1}^l) называется обучающей выборкой (training sample).
Задача обучения по прецедентам заключается в том, чтобы восстановить функциональную зависимость между объектами и ответами, то есть построить отображение ![](https://latex.codecogs.com/gif.latex?a:X&space;\rightarrow&space;Y) удовлетворяющее следующей совокупности требований:
- отображение должно допускать эффективную компьютерную реализацию. Отображение а - называется алгоритмом.
- алгоритм а(х) должен воспроизводить на объектах выборки заданные ответы: ![](https://latex.codecogs.com/gif.latex?a(x_i)=y_i). Равенство может быть точным или приближенным, в зависимости от задачи.
- На алгоритм a(x) могут накладываться разного рода априорные ограничения, например, требования непрерывности, гладкости, монотонности, и т. д., или сочетание нескольких требований. В некоторых случаях может задаваться модель алгоритма — ***функциональный вид отображения a(x)***, определённый с точностью до параметров.
- Алгоритм ![](https://latex.codecogs.com/gif.latex?a) должен обладать обобщающей способностью (generalization ability) - достаточно точно приближать целевую функцию ![](https://latex.codecogs.com/gif.latex?y^*(x)) на всём множестве X.
#### Разновидность задач обучения по прецедентам
В зависимости от множества У задачи обучения делятся на следующие типы:

- ![](https://latex.codecogs.com/gif.latex?Y%3D%5Cleft%20%5C%7B%201%2C%5Ccdots%20%2CM%20%5Cright%20%5C%7D) 
Задача классификации (classification) на M <ins>непересекающихся</ins> классов. В некоторых приложениях классы называют образами и говорят о задаче распознавания образов (pattern recognition). 
Алгоритм задачи классификации отвечает на вопрос - к какому классу принадлежит.
Пример: определить пациент болен или нет, письмо спам или нет.
- ![](https://latex.codecogs.com/gif.latex?Y%3D%5Cleft%20%5C%7B%200%2C1%20%5Cright%20%5C%7D)
M — задача классификации на M пересекающихся классов. В простейшем случае эта задача сводится к решению M независимых задач классификации с двумя непересекающимися классами.
Алгоритм задачи классификации отвечает на вопрос - к какому классу принадлежит.
Пример: классификация роз и шиповников, характеристики (побег, листья, соцветья и тд) остаются одинаковыми для двух классов, и какие-то параметры могут совпадать ( роза и шиповник из рода Шиповников).
- Y = R — задача восстановления регрессии (regression estimation).
Пример: определить стоимость проекта по параметрам: актуальность проекта, заинтересованность инвесторами, прогноз специалистов и тд.
- Задача прогнозирования (forecasting) является частным случаем классификации или восстановления регрессии, когда X — описание прошлого поведения объекта, Y — описание некоторых характеристик его будущего поведения.
Пример: спрогназировать акции компании или курс валюты, по имеющимся прошлым показаниям.

#### Объекты и признаки

Признак (feature) f объекта x — это результат измерения некоторой характеристики объекта. Формально признаком называется отображение ![](https://latex.codecogs.com/gif.latex?f:X\rightarrow&space;D_f), где ![](https://latex.codecogs.com/gif.latex?D_f) — множество допустимых значений признака. Любое отображение из множества Х можно рассматривать как признак. В частности, любой алгоритм ![](https://latex.codecogs.com/gif.latex?a:X\rightarrow&space;Y) также можно рассматривать как признак.

В зависимости от множества допустимых значений признаки делятся на следующие типы:
- *бинарный признак*:  ![](https://latex.codecogs.com/gif.latex?D_f%3D%5Cleft%20%5C%7B%200%2C1%20%5Cright%20%5C%7D)
- *номинальный*: ![](https://latex.codecogs.com/gif.latex?D_f) - конечное множество.
- *порядковый*: ![](https://latex.codecogs.com/gif.latex?D_f) - конечное упорядоченное множество.
- *количественный* ![](https://latex.codecogs.com/gif.latex?D_f=\mathbb{R})

Значениями признаков могут быть числовые последовательности, растровые изображения, функции, графы, результаты запросов к базе данных, и тд. Если все признаки имеют одинаковый тип, то исходные данные называются **однородными**, в противном случае - **разнородными**. 

Есть набор признаков ![](https://latex.codecogs.com/gif.latex?f_1,\cdots&space;,f_n).
![](https://latex.codecogs.com/gif.latex?%5Cleft%20%28f_1%28x%29%2C%5Ccdots%20%2Cf_n%28x%29%20%5Cright%20%29) - вектор признакового описания объекта ![](https://latex.codecogs.com/gif.latex?x\in&space;X). Совокупность признаковых описаний всех объектов выборки можно записать в матрицу (матрица объектов-признаков)
>Встречаются задачи, в которых данные устроены сложнее, например, описания объектов могут иметь переменную длину. В таких случаях по имеющимся исходным данным вычисляются преобразованные данные, имеющие стандартный вид. Этот приём называется извлечением признаков (features extraction) из данных.
#### Модель алгоритмов и метод обучения
Моделью алгоритмов называется параметрическое семейство отображений ![](https://latex.codecogs.com/gif.latex?A%3D%5Cleft%20%5C%7B%20g%28x%2C%5Ctheta%29%20%7C%20%5Ctheta%20%5Cin%20%5CTheta%20%5Cright%20%5C%7D%2C%20%5C%3A%20g%3AX%5Ctimes%20%5CTheta%20%5Crightarrow%20Y), g — некоторая фиксированная функция, ![](https://latex.codecogs.com/gif.latex?\Theta) — множество допустимых значений параметра ![](https://latex.codecogs.com/gif.latex?\theta), называемое пространством параметров или пространством поиска (search space).

Линейные модели с вектором параметров ![](https://latex.codecogs.com/gif.latex?%5Ctheta%3D%5Cleft%20%28%5Ctheta_1%2C%5Ccdots%20%2C%20%5Ctheta_n%20%5Cright%20%29%5Cin%20%5CTheta%20%3D%5Cmathbb%7BR%7D%5En):

- ![](https://latex.codecogs.com/gif.latex?g(x,\theta&space;)=\sum_{j=1}^{n}\theta_jf_j(x)) - для задачи восстановления регрессии, ![](https://latex.codecogs.com/gif.latex?Y=\mathbb{R}) 
- ![](https://latex.codecogs.com/gif.latex?g(x,\theta&space;)=sign\sum_{j=1}^{n}\theta_jf_j(x)) -для задач классификации ![](https://latex.codecogs.com/gif.latex?Y%3D%5Cleft%20%5C%7B%20-1%2C&plus;1%20%5Cright%20%5C%7D) 
>Признаки могут быть не только измерения, но и функции от них (выше)

Пример: если будем классифицировать выбокру из второго измерения (х_i,у_i) и введем n признаков, так чтобы ![](https://latex.codecogs.com/gif.latex?f_j(x)=x^{j-1}) по формулам g, то получим полином степени n-1.
Процесс подбора оптимального параметра модели ![](https://latex.codecogs.com/gif.latex?\theta) по обучающей выборке ![](https://latex.codecogs.com/gif.latex?X^l) называют настройкой (fitting) или обучением (training, learning) алгоритма ![](https://latex.codecogs.com/gif.latex?a\in&space;A).

**Методом обучения** называется отображение ![](https://latex.codecogs.com/gif.latex?\mu:(X\times&space;Y)^l\rightarrow&space;A), которое произвольной конечной выборке ![](https://latex.codecogs.com/gif.latex?X^l) ставит в соответствие алгоритм a: X → Y . Говорят также, что метод ![](https://latex.codecogs.com/gif.latex?\mu) строит алгоритм a по выборке ![](https://latex.codecogs.com/gif.latex?X^l). Метод обучения, как и сам алгоритм a, должен допускать эффективную программную реализацию.
Итак, в задачах обучения по прецедентам чётко различаются два этапа:
- на этапе обучения метод ![](https://latex.codecogs.com/gif.latex?\mu) по выборке ![](https://latex.codecogs.com/gif.latex?X^l) строит алгоритм ![](https://latex.codecogs.com/gif.latex?a=\mu&space;(X^l));
- на этапе применения алгоритму a подаются на вход новые объекты x, в общем
случае отличные от обучающих, для получения ответов y = a(x).

Этап обучения наиболее сложен. Как правило, он сводится к поиску параметров
модели, доставляющих оптимальное значение заданному функционалу качества.

#### Функционал качества
Функция потерь (loss function) — это неотрицательная функция L (a, x), характеризующая величину ошибки алгоритма a на объекте x. Если L (a, x) = 0, то ответ a(x) называется корректным.
Функционал качества алгоритма ![](https://latex.codecogs.com/gif.latex?a) на выборке ![](https://latex.codecogs.com/gif.latex?X^l):

![](https://latex.codecogs.com/gif.latex?Q(a,&space;X^l)=\frac{1}{l}\sum_{i=1}^{l}\mathfrak{L}(a,x_i))

Функционал Q называют также функционалом средних потерь или эмпирическим риском, так как он вычисляется по эмпирическим данным (xi, yi).
>Эмпирические данные — данные, полученные путём наблюдения или эксперимента.

Функция потерь, принимающая только значения 0 и 1, называется бинарной.
В этом случае L(a, x)=1 означает, что алгоритм a допускает ошибку на объекте x,
а функционал Q называется частотой ошибок алгоритма a на выборке ![](https://latex.codecogs.com/gif.latex?X^l).
Наиболее часто используются следующие функции потерь, при ![](https://latex.codecogs.com/gif.latex?Y\subseteq&space;\mathbb{R}):
- ![](https://latex.codecogs.com/gif.latex?\mathfrak{L}(a,x)=\left&space;[&space;a(x)\neq&space;y^*(x)&space;\right&space;]) — индикатор ошибки, обычно применяется в задачах
классификации (ответ будет 0 или 1);
- ![](https://latex.codecogs.com/gif.latex?\mathfrak{L}(a,x)=|&space;a(x)-&space;y^*(x)&space;|) — отклонение от правильного ответа; функционал Q называется средней ошибкой алгоритма ![](https://latex.codecogs.com/gif.latex?a) на выборке ![](https://latex.codecogs.com/gif.latex?X^l);
- ![](https://latex.codecogs.com/gif.latex?%5Cmathfrak%7BL%7D%28a%2Cx%29%3D%28a%28x%29-y%5E*%28x%29%29%5E2) — квадратичная функция потерь; функционал Q называется средней квадратичной ошибкой алгоритма ![](https://latex.codecogs.com/gif.latex?a) на выборке ![](https://latex.codecogs.com/gif.latex?X^l); обычно применяется в задачах регрессии.

Классический метод обучения, называемый минимизацией эмпирического риска (empirical risk minimization, ERM), заключается в том, чтобы найти в заданной модели A алгоритм ![](https://latex.codecogs.com/gif.latex?a), доставляющий минимальное значение функционалу качества Q на заданной обучающей выборке ![](https://latex.codecogs.com/gif.latex?X^l):
![](https://latex.codecogs.com/gif.latex?\mu&space;(X^l)=\arg&space;\min_{a&space;\in&space;A}&space;Q(a,X^l))
####  Вероятностная постановка задачи обучения
Данные могут быть неточными или неполными, тогда может возникнуть случай, когда одному и тому же описанию соответствуют различные объекты и различные ответы ![](https://latex.codecogs.com/gif.latex?\Rightarrow) ![](https://latex.codecogs.com/gif.latex?y^*(x)) (целевая зависимость) не является функцией (или просто неизвестна). 

РЕШЕНИЕ: устранение некорректности с помощью вероятностной постановка задачи. Вместо неизвестной целевой зависимости предполагаем, что  ![](https://latex.codecogs.com/gif.latex?\exists) неизвестное вероятностное распределение с плотностью  ![](https://latex.codecogs.com/gif.latex?p(x,y)=p(x)p(y|x)),![](https://latex.codecogs.com/gif.latex?p%28y%7Cx%29%3D%5Cdelta%28y-y%5E*%28x%29%29)  из которого случайно и независимо выбираются ![](https://latex.codecogs.com/gif.latex?l) наблюдений ![](https://latex.codecogs.com/gif.latex?X%5El%3D%28x_i%2Cy_i%29_%7Bi%3D1%7D%5El).

>![](https://latex.codecogs.com/gif.latex?\delta) - дельта-функция:
![](https://latex.codecogs.com/gif.latex?%5Cdelta%28x%29%3D%5Cleft%5C%7B%5Cbegin%7Bmatrix%7D%20&plus;%5Cinfty%20%2C%20%26%20x%3D0%20%5C%5C%200%2C%26%20x%5Cneq%200%20%5Cend%7Bmatrix%7D%5Cright.)

Такие выборки называются простыми или случайными одинаково распределёнными.
Вероятностная постановка задачи считается более общей, так как функциональную зависимость y.

#### Принцип максимума правдоподобия
Задаётся модель совместной плотности распределения объектов и ответов ![](https://latex.codecogs.com/gif.latex?\varphi(x,y,\theta)),
аппроксимирующая неизвестную плотность ![](https://latex.codecogs.com/gif.latex?p(x,y)).

>Аппроксимация - научный метод, состоящий в замене одних объектов другими, в каком-то смысле близкими к исходным, но более простыми

Затем определяется значение параметра ![](https://latex.codecogs.com/gif.latex?\theta), при котором выборка данных ![](https://latex.codecogs.com/gif.latex?X^l) максимально правдоподобна, то есть наилучшим образом согласуется с моделью плотности.
Если наблюдения в выборке ![](https://latex.codecogs.com/gif.latex?X^l) независимы, то совместная плотность распределения всех наблюдений равна произведению плотностей p(x, y) в каждом наблюдении. Подставляя вместо p(x, y) модель плотности ![](https://latex.codecogs.com/gif.latex?\varphi(x,y,\theta)) (аппроксимация), получаем функцию правдоподобия (likelihood):
![](https://latex.codecogs.com/gif.latex?L%28%5Ctheta%2C%20X%5El%29%3D%5Cprod_%7Bi%3D1%7D%5E%7Bl%7D%5Cvarphi%20%28x_i%2Cy_i%2C%5Ctheta%29.)

ПРИНЦИП МАКСИМУМА ПРАВДОПОДОБИЯ: чем выше значение правдоподобия, тем лучше выборка согласуется с моделью ![](https://latex.codecogs.com/gif.latex?\Rightarrow) найти такое значение ![](https://latex.codecogs.com/gif.latex?\theta), при котором значение функции правдоподобия максимально. Далее алгоритм строится по плотности ![](https://latex.codecogs.com/gif.latex?\varphi(x,y,\theta)).
#### Связь максимизации правдоподобия с минимизацией эмпирического риска
Вместо максимизации L удобнее минимизировать функционал  −ln(L), по объектам выборки из произведения превратится в сумму ( по свойствам логарифмов):

![](https://latex.codecogs.com/gif.latex?-%5Cln%20L%28%5Ctheta%2C%20X%5El%29%3D-%5Csum_%7Bi%3D1%7D%5E%7Bl%7D%5Cln%20%5Cvarphi%20%28x_i%2Cy_i%2C%5Ctheta%20%29%5Crightarrow%20%5Cmin_%7B%5Ctheta%20%7D)

Этот функционал совпадает с функционалом эмпирического риска, если определить вероятностную функцию потерь ![](https://latex.codecogs.com/gif.latex?\mathfrak{L}(a_\theta,&space;x)=-l\ln&space;\varphi&space;(x,y,\theta&space;))
Такое определение потери вполне естественно — чем хуже пара (xi, yi) согласуется с моделью ![](https://latex.codecogs.com/gif.latex?\varphi), тем меньше значение плотности  ![](https://latex.codecogs.com/gif.latex?\varphi(x_i,y_i,\theta)) и выше величина потери ![](https://latex.codecogs.com/gif.latex?\mathfrak{L}(a_\theta,&space;x)).
Верно и обратное — для многих функций потерь возможно подобрать модель плотности ![](https://latex.codecogs.com/gif.latex?\varphi(x,y,\theta))  таким образом, чтобы минимизация эмпирического риска была эквивалентна максимизации правдоподобия.
#### Проблема переобучения и понятие обобщающей способности
Минимум функционала качества алгоритма не гарантирует хороший результат для произвольной контрольной выборки. Когда качество работы алгоритма на новых объектах, не вошедших в состав обучения, оказывается существенно хуже, чем на обучающей выборке, говорят об эффекте **переобучения (overtraining) или переподгонки (overfitting)**.
>вероятность ошибки на тестовой выборке ![](https://latex.codecogs.com/gif.latex?>) (существенно выше) чем средняя ошибка на обучающей выборке.  
Переобучение возникает при использовании избыточно сложных моделей.

По сути алгоритм просто запоминает обучающуюся выборку ![](https://latex.codecogs.com/gif.latex?x_i\in&space;X^l): берет объект х и сравнивает с обучающей выборкой ![](https://latex.codecogs.com/gif.latex?x_i), если ![](https://latex.codecogs.com/gif.latex?x=x_i) - алгоритм выдаст правильный ответ, иначе - произвольный ответ.Однако этот алгоритм не способен восстановить зависимость вне материала обучения. Отсюда вывод: для успешного обучения необходимо не только запоминать, но и обобщать.

**Обобщающая способность (generalization ability, generalization performance).** Говорят, что алгоритм обучения обладает способностью к обобщению, если вероятность ошибки на тестовой выборке достаточно мала или хотя бы предсказуема, то есть не сильно отличается от ошибки на обучающей выборке. 
>вероятность ошибки на тестовой выборке ![](https://latex.codecogs.com/gif.latex?\approx) вероятность ошибки на обучающей выборке

**Недообучение** — нежелательное явление, возникающее при решении задач обучения по прецедентам, когда алгоритм обучения не обеспечивает достаточно малой величины средней ошибки на обучающей выборке. 
>вероятность ошибки на обучающей выборке ![](https://latex.codecogs.com/gif.latex?>&space;\varepsilon)
Недообучение возникает при использовании недостаточно сложных моделей.

Разобьём полную выборку ![](https://latex.codecogs.com/gif.latex?X^L) на две непересекающихся подвыборки: обучающую ![](https://latex.codecogs.com/gif.latex?X^l) и контрольную ![](https://latex.codecogs.com/gif.latex?X^k) , ![](https://latex.codecogs.com/gif.latex?L=k+l).
**Переобученностью** алгоритма ![](https://latex.codecogs.com/gif.latex?a=\mu(X^l)) на паре выборок ![](https://latex.codecogs.com/gif.latex?(X^l,X^k)) будем называть разность  ![](https://latex.codecogs.com/gif.latex?\delta&space;(\mu,&space;X^l,&space;X^k)=Q(a,X^k)-Q(a,X^l)).
Функционал **полного скользящего контроля (complete cross-validation)** определяется как средняя частота ошибок на контрольных подвыборках:
![](https://latex.codecogs.com/gif.latex?CVV(\mu,&space;X^L)=\frac{1}{N}\sum_{n=1}^{N}Q^k_n)
> ![](https://latex.codecogs.com/gif.latex?N) - от множества разбиений ![](https://latex.codecogs.com/gif.latex?%5Cleft%20%5C%7B%201%2C%5Ccdots%20%2C%20N%20%5Cright%20%5C%7D)
n - номер разбиения
![](https://latex.codecogs.com/gif.latex?Q%5Ek_n%3DQ%28%5Cmu%20%28X%5El_n%29%2C%20X%5Ek_n%29)
![](https://latex.codecogs.com/gif.latex?Q%5El_n%3DQ%28%5Cmu%20%28X%5El_n%29%2C%20X%5El_n%29)

ПРОБЛЕМА: разброс величины ![](https://latex.codecogs.com/gif.latex?Q^k_n) (частота ошбок мала, но значения ![](https://latex.codecogs.com/gif.latex?Q^k_n) большие для разбиения)


РЕШЕНИЕ: применение функции распределения ![](https://latex.codecogs.com/gif.latex?R%28%5Cmu%2C%20X%5El%29%3DP_n%5Cleft%20%5C%7B%20Q%5Ek_n%3E%5Cvarepsilon%20%5Cright%20%5C%7D%3D%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bn%3D1%7D%5E%7BN%7D%5Cleft%20%5B%20Q%5Ek_n%3E%5Cvarepsilon%20%5Cright%20%5D)


+ЕЩЁ РЕШЕНИЕ: иногда удобно расчитывать величину переобученности ![](https://latex.codecogs.com/gif.latex?Q_%5Cvarepsilon%20%28%5Cmu%2C%20X%5El%29%3DP_n%5Cleft%20%5C%7B%20%5Cdelta%20%28%5Cmu%2C%20X%5El_n%2CX%5Ek_n%29%3E%5Cvarepsilon%20%5Cright%20%5C%7D%3D%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bn%3D1%7D%5E%7BN%7D%5Cleft%20%5B%20Q%5Ek_n-Q%5El_n%3E%5Cvarepsilon%20%5Cright%20%5D)

Функционал ![](https://latex.codecogs.com/gif.latex?Q_\varepsilon) является кусочно-постоянной невозрастающей функцией параметра ![](https://latex.codecogs.com/gif.latex?\varepsilon). Пусть имеется его оценка сверху ![](https://latex.codecogs.com/gif.latex?Q_\varepsilon&space;\leqslant&space;\eta&space;(\varepsilon&space;)), где ![](https://latex.codecogs.com/gif.latex?\eta&space;(\varepsilon&space;)) — монотонно убывающая функция. Функция ![](https://latex.codecogs.com/gif.latex?\varepsilon(\eta)), обратная к ![](https://latex.codecogs.com/gif.latex?\eta&space;(\varepsilon&space;)), также монотонно убывающая. Тогда ![](https://latex.codecogs.com/gif.latex?Q_\varepsilon) эквивалентно утверждению, что для данного метода и выборки с вероятностью, не меньшей ![](https://latex.codecogs.com/gif.latex?1-\eta), выполняется неравенство ![](https://latex.codecogs.com/gif.latex?Q_n^k\leqslant&space;Q_n^l&plus;\varepsilon(\eta)). В этом случае говорят, что обучение ***состоятельно*** с точностью ![](https://latex.codecogs.com/gif.latex?\varepsilon) и надёжностью ![](https://latex.codecogs.com/gif.latex?\eta).

- Контроль по отдельным объектам (leave one out CV): k = 1
![](https://latex.codecogs.com/gif.latex?LOO%28%5Cmu%2C%20X%5EL%29%3D%5Cfrac%7B1%7D%7BL%7D%5Csum_%7Bi%3D1%7D%5E%7BL%7DQ%28X%5EL%20%5Csetminus%20%5Cleft%20%5C%7B%20x_i%20%5Cright%20%5C%7D%2C%5Cleft%20%5C%7B%20x_i%20%5Cright%20%5C%7D%29)


Проблема: ресурсоемкость

- Контроль по q блокам (q-fold CV): случайное разбиение ![](https://latex.codecogs.com/gif.latex?X%5EL%3DX%5E%7Bl_1%7D_1%5Ccup%20%5Ccdots%20%5Ccup%20X%5E%7Bl_q%7D_q) на q блоков (почти) равной длины
![](https://latex.codecogs.com/gif.latex?CV_q%28%5Cmu%2C%20X%5EL%29%3D%5Cfrac%7B1%7D%7Bq%7D%5Csum_%7Bi%3D1%7D%5E%7Bq%7DQ%28X%5EL%20%5Csetminus%20X%5E%7Bl_i%7D_i%2CX%5E%7Bl_i%7D_i%29)

Проблема:
- -  оценка существенно зависит от разбиения на блоки;
- - каждый объект лишь один раз участвует в контроле.


- Контроль t раз по q блокам (t×q-fold CV) — стандарт «де факто» для тестирования методов обучения.
Выборка X^L разбивается t раз случайным образом на q блоков
![](https://latex.codecogs.com/gif.latex?X%5EL%3DX%5E%7Bl_1%7D_%7Bs1%7D%5Ccup%20%5Ccdots%20%5Ccup%20X%5E%7Bl_q%7D_%7Bsq%7D%2C%5C%3B%20s%3D%5Coverline%7B1%2Ct%7D%2C%20%5C%3B%20l_1&plus;%5Ccdots%20&plus;l_q%3DL)
![](https://latex.codecogs.com/gif.latex?CV_%7Bt%5Ctimes%20q%7D%28%5Cmu%2C%20X%5EL%29%3D%5Cfrac%7B1%7D%7Bt%7D%5Csum_%7Bs%3D1%7D%5E%7Bt%7D%5Cfrac%7B1%7D%7Bq%7D%5Csum_%7Bn%3D1%7D%5E%7Bq%7DQ%28X%5EL%5Csetminus%20X%5E%7Bl_n%7D_%7Bsn%7D%2CX%5E%7Bl_n%7D_%7Bsn%7D%29)

Преимущества t×q-fold CV:
- - увеличением t можно улучшать точность оценки(компромисс между точностью и временем вычислений);
- - каждый объект участвует в контроле ровно t раз;
- - можно вычислять доверительные интервалы (при t > или прблизительно 20)

## 2. Модель алгоритмов и метод обучения. Функционал качества и функция потерь
 Моделью алгоритмов называется параметрическое семейство отображений ![](https://latex.codecogs.com/gif.latex?A%3D%5Cleft%20%5C%7B%20g%28x%2C%5Ctheta%29%20%7C%20%5Ctheta%20%5Cin%20%5CTheta%20%5Cright%20%5C%7D%2C%20%5C%3A%20g%3AX%5Ctimes%20%5CTheta%20%5Crightarrow%20Y), g — некоторая фиксированная функция, ![](https://latex.codecogs.com/gif.latex?\Theta) — множество допустимых значений параметра ![](https://latex.codecogs.com/gif.latex?\theta), называемое пространством параметров или пространством поиска (search space).

Линейные модели с вектором параметров ![](https://latex.codecogs.com/gif.latex?%5Ctheta%3D%5Cleft%20%28%5Ctheta_1%2C%5Ccdots%20%2C%20%5Ctheta_n%20%5Cright%20%29%5Cin%20%5CTheta%20%3D%5Cmathbb%7BR%7D%5En):

- ![](https://latex.codecogs.com/gif.latex?g(x,\theta&space;)=\sum_{j=1}^{n}\theta_jf_j(x)) - для задачи восстановления регрессии, ![](https://latex.codecogs.com/gif.latex?Y=\mathbb{R}) 
- ![](https://latex.codecogs.com/gif.latex?g(x,\theta&space;)=sign\sum_{j=1}^{n}\theta_jf_j(x)) -для задач классификации ![](https://latex.codecogs.com/gif.latex?Y%3D%5Cleft%20%5C%7B%20-1%2C&plus;1%20%5Cright%20%5C%7D) 
>Признаки могут быть не только измерения, но и функции от них (выше)

Пример: если будем классифицировать выбокру из второго измерения (х_i,у_i) и введем n признаков, так чтобы ![](https://latex.codecogs.com/gif.latex?f_j(x)=x^{j-1}) по формулам g, то получим полином степени n-1.
Процесс подбора оптимального параметра модели ![](https://latex.codecogs.com/gif.latex?\theta) по обучающей выборке ![](https://latex.codecogs.com/gif.latex?X^l) называют настройкой (fitting) или обучением (training, learning) алгоритма ![](https://latex.codecogs.com/gif.latex?a\in&space;A).

**Методом обучения** называется отображение ![](https://latex.codecogs.com/gif.latex?\mu:(X\times&space;Y)^l\rightarrow&space;A), которое произвольной конечной выборке ![](https://latex.codecogs.com/gif.latex?X^l) ставит в соответствие алгоритм a: X → Y . Говорят также, что метод ![](https://latex.codecogs.com/gif.latex?\mu) строит алгоритм a по выборке ![](https://latex.codecogs.com/gif.latex?X^l). Метод обучения, как и сам алгоритм a, должен допускать эффективную программную реализацию.
Итак, в задачах обучения по прецедентам чётко различаются два этапа:
- на этапе обучения метод ![](https://latex.codecogs.com/gif.latex?\mu) по выборке ![](https://latex.codecogs.com/gif.latex?X^l) строит алгоритм ![](https://latex.codecogs.com/gif.latex?a=\mu&space;(X^l));
- на этапе применения алгоритму a подаются на вход новые объекты x, в общем
случае отличные от обучающих, для получения ответов y = a(x).

Этап обучения наиболее сложен. Как правило, он сводится к поиску параметров
модели, доставляющих оптимальное значение заданному функционалу качества.

#### Функционал качества
Функция потерь (loss function) — это неотрицательная функция L (a, x), характеризующая величину ошибки алгоритма a на объекте x. Если L (a, x) = 0, то ответ a(x) называется корректным.
Функционал качества алгоритма ![](https://latex.codecogs.com/gif.latex?a) на выборке ![](https://latex.codecogs.com/gif.latex?X^l):

![](https://latex.codecogs.com/gif.latex?Q(a,&space;X^l)=\frac{1}{l}\sum_{i=1}^{l}\mathfrak{L}(a,x_i))

Функционал Q называют также функционалом средних потерь или эмпирическим риском, так как он вычисляется по эмпирическим данным (xi, yi).
>Эмпирические данные — данные, полученные путём наблюдения или эксперимента.

Функция потерь, принимающая только значения 0 и 1, называется бинарной.
В этом случае L(a, x)=1 означает, что алгоритм a допускает ошибку на объекте x,
а функционал Q называется частотой ошибок алгоритма a на выборке ![](https://latex.codecogs.com/gif.latex?X^l).
Наиболее часто используются следующие функции потерь, при ![](https://latex.codecogs.com/gif.latex?Y\subseteq&space;\mathbb{R}):
- ![](https://latex.codecogs.com/gif.latex?\mathfrak{L}(a,x)=\left&space;[&space;a(x)\neq&space;y^*(x)&space;\right&space;]) — индикатор ошибки, обычно применяется в задачах
классификации (ответ будет 0 или 1);
- ![](https://latex.codecogs.com/gif.latex?\mathfrak{L}(a,x)=|&space;a(x)-&space;y^*(x)&space;|) — отклонение от правильного ответа; функционал Q называется средней ошибкой алгоритма ![](https://latex.codecogs.com/gif.latex?a) на выборке ![](https://latex.codecogs.com/gif.latex?X^l);
- ![](https://latex.codecogs.com/gif.latex?%5Cmathfrak%7BL%7D%28a%2Cx%29%3D%28a%28x%29-y%5E*%28x%29%29%5E2) — квадратичная функция потерь; функционал Q называется средней квадратичной ошибкой алгоритма ![](https://latex.codecogs.com/gif.latex?a) на выборке ![](https://latex.codecogs.com/gif.latex?X^l); обычно применяется в задачах регрессии.

Классический метод обучения, называемый минимизацией эмпирического риска (empirical risk minimization, ERM), заключается в том, чтобы найти в заданной модели A алгоритм ![](https://latex.codecogs.com/gif.latex?a), доставляющий минимальное значение функционалу качества Q на заданной обучающей выборке ![](https://latex.codecogs.com/gif.latex?X^l):
![](https://latex.codecogs.com/gif.latex?\mu&space;(X^l)=\arg&space;\min_{a&space;\in&space;A}&space;Q(a,X^l))

## 3. Проблема переобучения и понятие обобщающей способности. Эмпирические оценки обобщающей способности. Разновидности скользящего контроля

Минимум функционала качества алгоритма не гарантирует хороший результат для произвольной контрольной выборки. Когда качество работы алгоритма на новых объектах, не вошедших в состав обучения, оказывается существенно хуже, чем на обучающей выборке, говорят об эффекте **переобучения (overtraining) или переподгонки (overfitting)**.
>вероятность ошибки на тестовой выборке ![](https://latex.codecogs.com/gif.latex?>) (существенно выше) чем средняя ошибка на обучающей выборке.  
Переобучение возникает при использовании избыточно сложных моделей.

По сути алгоритм просто запоминает обучающуюся выборку ![](https://latex.codecogs.com/gif.latex?x_i\in&space;X^l): берет объект х и сравнивает с обучающей выборкой ![](https://latex.codecogs.com/gif.latex?x_i), если ![](https://latex.codecogs.com/gif.latex?x=x_i) - алгоритм выдаст правильный ответ, иначе - произвольный ответ.Однако этот алгоритм не способен восстановить зависимость вне материала обучения. Отсюда вывод: для успешного обучения необходимо не только запоминать, но и обобщать.

**Обобщающая способность (generalization ability, generalization performance).** Говорят, что алгоритм обучения обладает способностью к обобщению, если вероятность ошибки на тестовой выборке достаточно мала или хотя бы предсказуема, то есть не сильно отличается от ошибки на обучающей выборке. 
>вероятность ошибки на тестовой выборке ![](https://latex.codecogs.com/gif.latex?\approx) вероятность ошибки на обучающей выборке

**Недообучение** — нежелательное явление, возникающее при решении задач обучения по прецедентам, когда алгоритм обучения не обеспечивает достаточно малой величины средней ошибки на обучающей выборке. 
>вероятность ошибки на обучающей выборке ![](https://latex.codecogs.com/gif.latex?>&space;\varepsilon)
Недообучение возникает при использовании недостаточно сложных моделей.

Разобьём полную выборку ![](https://latex.codecogs.com/gif.latex?X^L) на две непересекающихся подвыборки: обучающую ![](https://latex.codecogs.com/gif.latex?X^l) и контрольную ![](https://latex.codecogs.com/gif.latex?X^k) , ![](https://latex.codecogs.com/gif.latex?L=k+l).
**Переобученностью** алгоритма ![](https://latex.codecogs.com/gif.latex?a=\mu(X^l)) на паре выборок ![](https://latex.codecogs.com/gif.latex?(X^l,X^k)) будем называть разность  ![](https://latex.codecogs.com/gif.latex?\delta&space;(\mu,&space;X^l,&space;X^k)=Q(a,X^k)-Q(a,X^l)).
Функционал **полного скользящего контроля (complete cross-validation)** определяется как средняя частота ошибок на контрольных подвыборках:
![](https://latex.codecogs.com/gif.latex?CVV(\mu,&space;X^L)=\frac{1}{N}\sum_{n=1}^{N}Q^k_n)
> ![](https://latex.codecogs.com/gif.latex?N) - от множества разбиений ![](https://latex.codecogs.com/gif.latex?%5Cleft%20%5C%7B%201%2C%5Ccdots%20%2C%20N%20%5Cright%20%5C%7D)
n - номер разбиения
![](https://latex.codecogs.com/gif.latex?Q%5Ek_n%3DQ%28%5Cmu%20%28X%5El_n%29%2C%20X%5Ek_n%29)
![](https://latex.codecogs.com/gif.latex?Q%5El_n%3DQ%28%5Cmu%20%28X%5El_n%29%2C%20X%5El_n%29)

ПРОБЛЕМА: разброс величины ![](https://latex.codecogs.com/gif.latex?Q^k_n) (частота ошбок мала, но значения ![](https://latex.codecogs.com/gif.latex?Q^k_n) большие для разбиения)


РЕШЕНИЕ: применение функции распределения ![](https://latex.codecogs.com/gif.latex?R%28%5Cmu%2C%20X%5El%29%3DP_n%5Cleft%20%5C%7B%20Q%5Ek_n%3E%5Cvarepsilon%20%5Cright%20%5C%7D%3D%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bn%3D1%7D%5E%7BN%7D%5Cleft%20%5B%20Q%5Ek_n%3E%5Cvarepsilon%20%5Cright%20%5D)


+ЕЩЁ РЕШЕНИЕ: иногда удобно расчитывать величину переобученности ![](https://latex.codecogs.com/gif.latex?Q_%5Cvarepsilon%20%28%5Cmu%2C%20X%5El%29%3DP_n%5Cleft%20%5C%7B%20%5Cdelta%20%28%5Cmu%2C%20X%5El_n%2CX%5Ek_n%29%3E%5Cvarepsilon%20%5Cright%20%5C%7D%3D%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bn%3D1%7D%5E%7BN%7D%5Cleft%20%5B%20Q%5Ek_n-Q%5El_n%3E%5Cvarepsilon%20%5Cright%20%5D)

Функционал ![](https://latex.codecogs.com/gif.latex?Q_\varepsilon) является кусочно-постоянной невозрастающей функцией параметра ![](https://latex.codecogs.com/gif.latex?\varepsilon). Пусть имеется его оценка сверху ![](https://latex.codecogs.com/gif.latex?Q_\varepsilon&space;\leqslant&space;\eta&space;(\varepsilon&space;)), где ![](https://latex.codecogs.com/gif.latex?\eta&space;(\varepsilon&space;)) — монотонно убывающая функция. Функция ![](https://latex.codecogs.com/gif.latex?\varepsilon(\eta)), обратная к ![](https://latex.codecogs.com/gif.latex?\eta&space;(\varepsilon&space;)), также монотонно убывающая. Тогда ![](https://latex.codecogs.com/gif.latex?Q_\varepsilon) эквивалентно утверждению, что для данного метода и выборки с вероятностью, не меньшей ![](https://latex.codecogs.com/gif.latex?1-\eta), выполняется неравенство ![](https://latex.codecogs.com/gif.latex?Q_n^k\leqslant&space;Q_n^l&plus;\varepsilon(\eta)). В этом случае говорят, что обучение ***состоятельно*** с точностью ![](https://latex.codecogs.com/gif.latex?\varepsilon) и надёжностью ![](https://latex.codecogs.com/gif.latex?\eta).

- Контроль по отдельным объектам (leave one out CV): k = 1
![](https://latex.codecogs.com/gif.latex?LOO%28%5Cmu%2C%20X%5EL%29%3D%5Cfrac%7B1%7D%7BL%7D%5Csum_%7Bi%3D1%7D%5E%7BL%7DQ%28X%5EL%20%5Csetminus%20%5Cleft%20%5C%7B%20x_i%20%5Cright%20%5C%7D%2C%5Cleft%20%5C%7B%20x_i%20%5Cright%20%5C%7D%29)


Проблема: ресурсоемкость

- Контроль по q блокам (q-fold CV): случайное разбиение ![](https://latex.codecogs.com/gif.latex?X%5EL%3DX%5E%7Bl_1%7D_1%5Ccup%20%5Ccdots%20%5Ccup%20X%5E%7Bl_q%7D_q) на q блоков (почти) равной длины
![](https://latex.codecogs.com/gif.latex?CV_q%28%5Cmu%2C%20X%5EL%29%3D%5Cfrac%7B1%7D%7Bq%7D%5Csum_%7Bi%3D1%7D%5E%7Bq%7DQ%28X%5EL%20%5Csetminus%20X%5E%7Bl_i%7D_i%2CX%5E%7Bl_i%7D_i%29)

Проблема:
- -  оценка существенно зависит от разбиения на блоки;
- - каждый объект лишь один раз участвует в контроле.


- Контроль t раз по q блокам (t×q-fold CV) — стандарт «де факто» для тестирования методов обучения.
Выборка X^L разбивается t раз случайным образом на q блоков
![](https://latex.codecogs.com/gif.latex?X%5EL%3DX%5E%7Bl_1%7D_%7Bs1%7D%5Ccup%20%5Ccdots%20%5Ccup%20X%5E%7Bl_q%7D_%7Bsq%7D%2C%5C%3B%20s%3D%5Coverline%7B1%2Ct%7D%2C%20%5C%3B%20l_1&plus;%5Ccdots%20&plus;l_q%3DL)
![](https://latex.codecogs.com/gif.latex?CV_%7Bt%5Ctimes%20q%7D%28%5Cmu%2C%20X%5EL%29%3D%5Cfrac%7B1%7D%7Bt%7D%5Csum_%7Bs%3D1%7D%5E%7Bt%7D%5Cfrac%7B1%7D%7Bq%7D%5Csum_%7Bn%3D1%7D%5E%7Bq%7DQ%28X%5EL%5Csetminus%20X%5E%7Bl_n%7D_%7Bsn%7D%2CX%5E%7Bl_n%7D_%7Bsn%7D%29)

Преимущества t×q-fold CV:
- - увеличением t можно улучшать точность оценки(компромисс между точностью и временем вычислений);
- - каждый объект участвует в контроле ровно t раз;
- - можно вычислять доверительные интервалы (при t > или прблизительно 20)

## 4-6 Метрические алгоритмы классификации
 
 Метрический классификатор (similarity-based classifier) — алгоритм классификации, основанный на вычислении оценок сходства между объектами. Метрические классификаторы опираются на **гипотезу компактности**, которая предполагает, что схожие объекты чаще лежат в одном классе, чем в разных. Это означает, что граница между классами имеет достаточно простую форму, и классы образуют компактно локализованные области в пространстве объектов. Заметим, что в математическом анализе компактными называются ограниченные замкнутые множества.
 ПРОБЛЕМА ВЫБОРА МЕТРИКИ:
 - редко когда работает евклидова метрика ( выбор евклидовой ничем не обоснован, просто проще)
 - нормирование (нужно ли?)
 - проблема проклятия размерности: больше размерность - меньше достоверность работы алгоритма ([тут подробней про саму проблему](https://vc.ru/ml/137209-proklyate-razmernosti))
  

## 4. Метрические алгоритмы классификации: алгоритмы ближайших соседей

###  Обобщённый метрический классификатор
Для произвольного объекта ![](https://latex.codecogs.com/gif.latex?u&space;\in&space;X) расположим элементы обучающей выборки ![](https://latex.codecogs.com/gif.latex?x_1,\dots,x_l) в порядке возрастания расстояний до ![](https://latex.codecogs.com/gif.latex?u):

![](https://latex.codecogs.com/gif.latex?%5Crho%20%28u%2Cx_u%5E%7B%281%29%7D%29%5Cleqslant%20%5Crho%20%28u%2Cx_u%5E%7B%282%29%7D%29%5Cleqslant%20%5Ccdots%20%5Cleqslant%20%5Crho%20%28u%2Cx_u%5E%7B%28l%29%7D%29)

где через ![](https://latex.codecogs.com/gif.latex?x_u^{(i)}) обозначается i-й сосед объекта u.

**Метрический алгоритм классификации** с обучающей выборкой ![](https://latex.codecogs.com/gif.latex?X^l) относит объект ![](https://latex.codecogs.com/gif.latex?u) к тому классу y, для которого суммарный вес ближайших обучающих объектов ![](https://latex.codecogs.com/gif.latex?\Gamma_y(u,X^l)) максимален:

![](https://latex.codecogs.com/gif.latex?a%28u%2C%20X%5El%29%3D%5Carg%20%5Cmax_%7By%5Cin%20Y%7D%5CGamma_y%28u%2CX%5El%29%3B%5C%3B%20%5C%3A%20%5C%3A%20%5C%3A%20%5C%3B%20%5C%3B%20%5C%3B%20%5C%3B%20%5C%3B%20%5C%3B%20%5C%3B%20%5C%3A%20%5C%3A%20%5C%3A%20%5C%3A%20%5C%3A%20%5CGamma_y%28u%2CX%5El%29%3D%5Csum_%7Bi%3D1%7D%5E%7Bl%7D%5Cleft%20%5B%20y_u%5E%7B%28i%29%7D%3Dy%20%5Cright%20%5Dw%28i%2Cu%29)

где весовая функция w(i, u) оценивает степень важности i-го соседа для классификации объекта u. Функция ![](https://latex.codecogs.com/gif.latex?\Gamma_y(u,X^l)) называется оценкой близости объекта u к классу y.

Метрический классификатор определён с точностью до весовой функции w(i, u) (функция w неотрицательна). Обучающая выборка параметр алгоритма а. Алгоритм делает аппроксимацию выборки, причем вычисления не производятся до тех пор, пока не известен объект  u. ---> относится к методам ленивого обучения и методам рассуждения по прецедентам.

### Алгоритм ближайших соседей

Алгоритм ближайшего соседа (nearest neighbor, NN) относит классифицируемый объект ![](https://latex.codecogs.com/gif.latex?u&space;\in&space;X^l) к тому классу, которому принадлежит ближайший обучающий объект:
![](https://latex.codecogs.com/gif.latex?w%28i%2Cu%29%3D%5Bi%3D1%5D%3B%20%5C%3B%20%5C%3B%20%5C%3B%20%5C%3B%20%5C%3B%20%5C%3B%20%5C%3B%20%5C%3B%20%5C%3B%20%5C%3B%20a%28u%2CX%5El%29%3Dy_u%5E%7B%281%29%7D)
Преимущество: простота реализации
Недостатки:
- неустойчивость к погрешностям (выбрсы повлияют на работу)
- отсутствие параметров, которые можно было настраивать по метрике. (алгоритм зависит от того, насколько удачно выбрана метрика)
- низкое качество классификации

### Алгоритм k ближайших соседей (k nearest neighbors, kNN). 
Чтобы сгладить влияние выбросов, будем относить объект u к тому классу, элементов которого окажется больше среди k ближайших соседей ![](https://latex.codecogs.com/gif.latex?x_u%5E%7B%28i%29%7D%2C%20%5C%3B%20%5C%3B%20%5C%3B%20%5C%3B%20i%3D1%2C%5Ccdots%20%2Ck%3B):

![](https://latex.codecogs.com/gif.latex?w%28i%2Cu%29%3D%5Cleft%20%5B%20i%5Cleqslant%20k%20%5Cright%20%5D%3B%5C%3B%20%5C%3B%20%5C%3B%20%5C%3B%20%5C%3B%20%5C%3B%20%5C%3B%20%5C%3B%20a%28u%2CX%5El%2Ck%29%3D%5Carg%5Cmax_%7By%20%5Cin%20Y%7D%5Csum_%7Bi%3D1%7D%5E%7Bk%7D%5Cleft%20%5B%20y_u%5E%7B%28i%29%7D%3Dy%20%5Cright%20%5D)

При k=1 - неустойчив к шуму, при k=l - устойчив и выражается в константу. Оптимальное значение параметра k определяют по критерию скользящего контроля с исключением объектов по одному (leave-one-out, LOO). Для каждого объекта ![](https://latex.codecogs.com/gif.latex?x_i&space;\in&space;X^l) проверяется, правильно ли он классифицируется по своим k ближайшим соседям.

![](https://latex.codecogs.com/gif.latex?LOO%28k%2CX%5El%29%3D%5Csum_%7Bi%3D1%7D%5E%7Bl%7D%5Cleft%20%5B%20a%28x_i%2C%20X%5El%5Csetminus%20%5Cleft%20%5C%7B%20x_i%20%5Cright%20%5C%7D%2Ck%29%5Cneq%20y_i%5Cright%20%5D%5Crightarrow%20%5Cmin_%7Bk%7D)

Альтернатива: каждом классе выбирается k ближайших к u объектов, и объект u относится к тому классу, для которого среднее расстояние до k ближайших соседей минимально.
Достоинство: учёт выбросов ( стараемся их сгладить)
Недостаток: максимум может достигаться на нескольких классах

### Алгоритм k взвешенных ближайших соседей
Вводим убывающую последовательность вещественных весов w_i, задающих вклад i-го соседа в классификацию:

![](https://latex.codecogs.com/gif.latex?w%28i%2Cu%29%3D%5Cleft%20%5B%20i%5Cleqslant%20k%20%5Cright%20%5Dw_i%3B%5C%3B%20%5C%3B%20%5C%3B%20%5C%3B%20%5C%3B%20%5C%3B%20%5C%3B%20%5C%3B%20a%28u%2CX%5El%2Ck%29%3D%5Carg%5Cmax_%7By%20%5Cin%20Y%7D%5Csum_%7Bi%3D1%7D%5E%7Bk%7D%5Cleft%20%5B%20y_u%5E%7B%28i%29%7D%3Dy%20%5Cright%20%5Dw_i)
 
выбор ![](https://latex.codecogs.com/gif.latex?w_i) --- эвристика. Возьмем линейно убывающие веса ![](https://latex.codecogs.com/gif.latex?w_i=\frac{k&plus;1-i}{k}) , то неоднозначности также могут возникать, хотя и реже (пример: классов два; первый и четвёртый сосед голосуют за класс 1, второй и третий — за класс 2; суммы голосов совпадают) ---> устранение: введем геометрическую прогрессию ![](https://latex.codecogs.com/gif.latex?w_i=q^i), где знаменатель прогрессии ![](https://latex.codecogs.com/gif.latex?q\in(0,1)) является параметром алгоритма. Его можно подбирать по критерию LOO, аналогично числу соседей k.

Недостатки простейших метрических алгоритмов типа kNN.
- Приходится хранить обучающую выборку целиком. Это приводит к неэффективному расходу памяти и чрезмерному усложнению решающего правила.
При наличии погрешностей (как в исходных данных, так и в модели сходства ρ)
это может приводить к понижению точности классификации вблизи границы
классов. Имеет смысл отбирать минимальное подмножество эталонных объектов, действительно необходимых для классификации.
- Поиск ближайшего соседа предполагает сравнение классифицируемого объекта
со всеми объектами выборки за O(ℓ) операций. Для задач с большими выборками или высокой частотой запросов это может оказаться накладно. Проблема решается с помощью эффективных алгоритмов поиска ближайших соседей,
требующих в среднем O(ln ℓ) операций.
- В простейших случаях метрические алгоритмы имеют крайне бедный набор
параметров, что исключает возможность настройки алгоритма по данным.
