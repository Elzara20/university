# Метод опорных вектов (SVM, Support Vector Machine)
Метод применяется для решения задач классификации и регрессии. Основа алгоритма данного метода составляет то, что обучающие объекты должны быть удалены от разделяющей поверхности настолько далеко, насколько это возможно.

*Главные преимущества алгоритма:*
- сводится к задаче квадратичного программирования (имеет единственное решение)
- положение разделяющей гиперплоскости зависит от некоторых объектов ( опорных векторов)
- внедрение функции ядра (возможность образовывать нелинейных разделяющих поверхностей)

## Линейно разделимая выборка
Пусть выборка линейно разделима, то есть существует некоторая гиперплоскость, разделяющая классы −1 и +1. Тогда *линейный пороговый классификатор*:

![](https://latex.codecogs.com/gif.latex?a(\vec{x})=sign(\langle&space;\vec{w},&space;\vec{x}&space;\rangle&space;-&space;b)=sign(\sum_{i=1}^lw_ix_i-b))

где   ![](https://latex.codecogs.com/gif.latex?\vec{w}=(w_1,\dots,w_n)\in&space;\mathbb{R}^n,&space;b&space;\in&space;\mathbb{R})


Для двух линейно разделимых классов возможны различные варианты построения разделяющих гиперплоскостей. 
Пример:

![](https://neerc.ifmo.ru/wiki/images/f/fe/Svm_hyperplane.png)


Метод опорных векторов выбирает ту гиперплоскость, которая максимизирует отступ между классами
>Отступ (англ. margin) — характеристика, оценивающая, насколько объект "погружён" в свой класс, насколько типичным представителем класса он является.  Максимизация зазора  между классами должна способствовать более надёжной классификации.
Формула:
![](https://latex.codecogs.com/gif.latex?M_i(\vec{w},b)=y_i(\langle\vec{w},\vec{x}_i\rangle-b))

Если выборка линейно разделима, то существует такая гиперплоскость, отступ от которой до каждого объекта положителен, объекты обучающей выборки находились на наибольшем расстоянии от неё.

Обозначим любой "крайний" объект из класса +1 как ![](https://latex.codecogs.com/gif.latex?\vec{x}_&plus;), а из -1     ![](https://latex.codecogs.com/gif.latex?\&space;\vec{x}_-). Их отступ равен единице. 

Нормировка позволяет ограничить разделяющую полосу между классами: ![](https://latex.codecogs.com/gif.latex?\lbrace&space;x:-1<\langle\vec{w},\vec{x}_i\rangle-b<1&space;\rbrace). Внутри неё не может лежать ни один объект обучающей выборки. Чтобы разделяющая гиперплоскость находилась на наибольшем расстоянии от точек выборки, ширина полосы должна быть максимальной. Это приводит нас к постановке задачи оптимизации в терминах квадратичного программирования:


![](https://latex.codecogs.com/gif.latex?%5Cbegin%7Bcases%7D%20%5C%7C%5Cvec%7Bw%7D%5C%7C%5E2%20%5Cto%20%5Cmin_%7Bw%2Cb%7D%5C%5C%20M_i%28%5Cvec%7Bw%7D%2Cb%29%5Cgeq%201%2C%20%26%20i%3D1%2C%5Cdots%2Cl%20%5Cend%7Bcases%7D) 
## Линейно неразделимая выборка
Необходимо ослабить ограничения, позволив некоторым объектам попадать на "территорию" другого класса. Для каждого объекта отнимем от отступа некоторую положительную величину ![](https://latex.codecogs.com/gif.latex?\xi_i), но потребуем чтобы эти введённые поправки были минимальны. Это приведёт к следующей постановке задачи, называемой также SVM с мягким отступом (англ. soft-margin SVM):

![](https://latex.codecogs.com/gif.latex?%5Cbegin%7Bcases%7D%20%5Cfrac%7B1%7D%7B2%7D%5C%7C%5Cvec%7Bw%7D%5C%7C%5E2%20&plus;%20C%5Csum_%7Bi%3D1%7D%5El%20%5Cxi_i%20%5Cto%20%5Cmin_%7Bw%2Cb%2C%5Cxi%7D%5C%5C%20M_i%28%5Cvec%7Bw%7D%2Cb%29%5Cgeq%201-%5Cxi%2C%26%20i%3D1%2C%5Cdots%2Cl%20%5C%5C%20%5Cxi%5Cgeq%200%2C%20%26%20i%3D1%2C%5Cdots%2Cl%20%5Cend%7Bcases%7D)


Мы можем упростить постановку задачи и получим эквивалентную задачу безусловной минимизации:

![](https://latex.codecogs.com/gif.latex?%5Cfrac%7B1%7D%7B2%7D%5C%7C%5Cvec%7Bw%7D%5C%7C%5E2%20&plus;%20C%5Csum_%7Bi%3D1%7D%5El%20%281-M_i%28%5Cvec%7Bw%7D%2Cb%29%29_&plus;%20%5Cto%20%5Cmin_%7Bw%2Cb%7D%5C%5C)

Пусть поставлена задача нелинейного программирования с ограничениями. Если x — точка локального минимума при наложенных ограничениях, то существуют такие множители ![](https://latex.codecogs.com/gif.latex?\mu_i), ![](https://latex.codecogs.com/gif.latex?\lambda_j), что для функции Лагранжа выполняются условия:
- условия дополняющей нежёсткости
- производные равны нулю


Диапазон значений ![](https://latex.codecogs.com/gif.latex?\lambda_i) позволяет нам разделить объекты обучающей выборки на три типа:

- ![](https://latex.codecogs.com/gif.latex?\lambda_i=0)— периферийные (неинформативные) объекты
Эти объекты лежат в своём классе, классифицируются верно и не влияют на выбор разделяющей гиперплоскости
- ![](https://latex.codecogs.com/gif.latex?0<\lambda_i<C)— опорные граничные объекты. Эти объекты лежат ровно на границе разделяющей полосы на стороне своего класса
- ![](https://latex.codecogs.com/gif.latex?\lambda_i=C)— опорные объекты-нарушители. Эти объекты лежат внутри разделяющей полосы или на стороне чужого класса


## Ядро

Пространство H должно быть наделено
скалярным произведением, в частности, подойдёт любое евклидово, а в общем случае
и гильбертово, пространство. Это даёт возможность заменить скалярное произведение в пространстве X на ядро — функцию, являющуюся скалярным произведением в некотором H.
## ROC-кривая
Рассмотрим задачу классификации на два класса, ![](https://latex.codecogs.com/gif.latex?Y%3D%5Cleft%20%5C%7B%20-1%3B&plus;1%20%5Cright%20%5C%7D), и модель
алгоритмов ![](https://latex.codecogs.com/gif.latex?a%28x%2C%20w%29%3Dsign%28f%28x%2C%20w%29%20-%20w_0%29), где ![](https://latex.codecogs.com/gif.latex?w_0%20%5Cin%20%5Cmathbb%7BR%7D) — аддитивный параметр дискриминантной функции. В теории нейронных сетей его называют порогом активации.
В случае линейной дискриминантной функции параметр ![](https://latex.codecogs.com/gif.latex?w_0) определяется отношением потерь:

![](https://latex.codecogs.com/gif.latex?w_0%20%3D%5Cln%20%5Cfrac%7B-%5Clambda%20%7D%7B&plus;%5Clambda%20%7D)


где λ+ и λ− — величина потери
при ошибке на объекте класса «+1» и «−1» соответственно.
На практике отношение потерь может многократно пересматриваться. Поэтому
вводится специальная характеристика — ROC-кривая, которая показывает, что происходит с числом ошибок обоих типов, если изменяется отношение потерь.













