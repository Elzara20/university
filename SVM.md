# Метод опорных вектов (SVM, Support Vector Machine)
Метод применяется для решения задач классификации и регрессии. Основа алгоритма данного метода составляет то, что обучающие объекты должны быть удалены от разделяющей поверхности настолько далеко, насколько это возможно.

*Главные преимущества алгоритма:*
- сводится к задаче квадратичного программирования (имеет единственное решение)
- положение разделяющей гиперплоскости зависит от некоторых объектов ( опорных векторов)
- внедрение функции ядра (возможность образовывать нелинейных разделяющих поверхностей)

## Линейно разделимая выборка
Пусть выборка линейно разделима, то есть существует некоторая гиперплоскость, разделяющая классы −1 и +1. Тогда *линейный пороговый классификатор*:

![](https://latex.codecogs.com/gif.latex?a(\vec{x})=sign(\langle&space;\vec{w},&space;\vec{x}&space;\rangle&space;-&space;b)=sign(\sum_{i=1}^lw_ix_i-b))

где   ![](https://latex.codecogs.com/gif.latex?\vec{w}=(w_1,\dots,w_n)\in&space;\mathbb{R}^n,&space;b&space;\in&space;\mathbb{R})


Для двух линейно разделимых классов возможны различные варианты построения разделяющих гиперплоскостей. 
Пример:

![](https://neerc.ifmo.ru/wiki/images/f/fe/Svm_hyperplane.png)


Метод опорных векторов выбирает ту гиперплоскость, которая максимизирует отступ между классами
>Отступ (англ. margin) — характеристика, оценивающая, насколько объект "погружён" в свой класс, насколько типичным представителем класса он является.  Максимизация зазора  между классами должна способствовать более надёжной классификации.
Формула:
![](https://latex.codecogs.com/gif.latex?M_i(\vec{w},b)=y_i(\langle\vec{w},\vec{x}_i\rangle-b))

Если выборка линейно разделима, то существует такая гиперплоскость, отступ от которой до каждого объекта положителен, объекты обучающей выборки находились на наибольшем расстоянии от неё.

Обозначим любой "крайний" объект из класса +1 как ![](https://latex.codecogs.com/gif.latex?\vec{x}_&plus;), а из -1     ![](https://latex.codecogs.com/gif.latex?\&space;\vec{x}_-). Их отступ равен единице. 

Нормировка позволяет ограничить разделяющую полосу между классами: ![](https://latex.codecogs.com/gif.latex?\lbrace&space;x:-1<\langle\vec{w},\vec{x}_i\rangle-b<1&space;\rbrace). Внутри неё не может лежать ни один объект обучающей выборки. Чтобы разделяющая гиперплоскость находилась на наибольшем расстоянии от точек выборки, ширина полосы должна быть максимальной. Это приводит нас к постановке задачи оптимизации в терминах квадратичного программирования:


![](https://latex.codecogs.com/gif.latex?%5Cbegin%7Bcases%7D%20%5C%7C%5Cvec%7Bw%7D%5C%7C%5E2%20%5Cto%20%5Cmin_%7Bw%2Cb%7D%5C%5C%20M_i%28%5Cvec%7Bw%7D%2Cb%29%5Cgeq%201%2C%20%26%20i%3D1%2C%5Cdots%2Cl%20%5Cend%7Bcases%7D) 
## Линейно неразделимая выборка
Необходимо ослабить ограничения, позволив некоторым объектам попадать на "территорию" другого класса. Для каждого объекта отнимем от отступа некоторую положительную величину ![](https://latex.codecogs.com/gif.latex?\xi_i), но потребуем чтобы эти введённые поправки были минимальны. Это приведёт к следующей постановке задачи, называемой также SVM с мягким отступом (англ. soft-margin SVM):

![](https://latex.codecogs.com/gif.latex?%5Cbegin%7Bcases%7D%20%5Cfrac%7B1%7D%7B2%7D%5C%7C%5Cvec%7Bw%7D%5C%7C%5E2%20&plus;%20C%5Csum_%7Bi%3D1%7D%5El%20%5Cxi_i%20%5Cto%20%5Cmin_%7Bw%2Cb%2C%5Cxi%7D%5C%5C%20M_i%28%5Cvec%7Bw%7D%2Cb%29%5Cgeq%201-%5Cxi%2C%26%20i%3D1%2C%5Cdots%2Cl%20%5C%5C%20%5Cxi%5Cgeq%200%2C%20%26%20i%3D1%2C%5Cdots%2Cl%20%5Cend%7Bcases%7D)


Мы можем упростить постановку задачи и получим эквивалентную задачу безусловной минимизации:

![](https://latex.codecogs.com/gif.latex?%5Cfrac%7B1%7D%7B2%7D%5C%7C%5Cvec%7Bw%7D%5C%7C%5E2%20&plus;%20C%5Csum_%7Bi%3D1%7D%5El%20%281-M_i%28%5Cvec%7Bw%7D%2Cb%29%29_&plus;%20%5Cto%20%5Cmin_%7Bw%2Cb%7D%5C%5C)

Пусть поставлена задача нелинейного программирования с ограничениями. Если x — точка локального минимума при наложенных ограничениях, то существуют такие множители ![](https://latex.codecogs.com/gif.latex?\mu_i), ![](https://latex.codecogs.com/gif.latex?\lambda_j), что для функции Лагранжа выполняются условия:
- условия дополняющей нежёсткости
- производные равны нулю


Диапазон значений ![](https://latex.codecogs.com/gif.latex?\lambda_i) позволяет нам разделить объекты обучающей выборки на три типа:

- ![](https://latex.codecogs.com/gif.latex?\lambda_i=0)— периферийные (неинформативные) объекты
Эти объекты лежат в своём классе, классифицируются верно и не влияют на выбор разделяющей гиперплоскости
- ![](https://latex.codecogs.com/gif.latex?0<\lambda_i<C)— опорные граничные объекты. Эти объекты лежат ровно на границе разделяющей полосы на стороне своего класса
- ![](https://latex.codecogs.com/gif.latex?\lambda_i=C)— опорные объекты-нарушители. Эти объекты лежат внутри разделяющей полосы или на стороне чужого класса

Алгоритм работы SVM с помощью библиотеки kernlab:

1. С помощью функции ksvm строим SVM модель, из которой будем получать необходимые данные
2. Находим  веса:
    1. умножаем коэффициенты, полученные из значения coef(), на точки принадлежащие этим коэфициентам
    2. в полученной матрице складываем столбцы
3. Находим отрицательное пересечение из значения b()
4. Нормализируем данные с помощью значения scale применяя функцию sapply
5. Рисуем прямую ![](https://latex.codecogs.com/gif.latex?y=kx-b), которая соответствует оптимальному разделению:
    1. пересечение или сдвиг (b в уравнении) равен ```b(svm_model)/sqrt(sum(w^2))``` (используем именно эту формулу так как это показывает модуль расстоянию от гиперплоскости до начала координат.)
    2. угол наклона будет равен ```sqrt(sum(w^2))``` (норма вектора w)
```R
data=data.frame(x=iris[30:40,1], y=iris[30:40,2], class=iris[30:40,5])
data= rbind(data,data.frame(x=iris[50:60,1], y=iris[50:60,2], class=iris[50:60,5]))

svm_model= ksvm(class ~ x+ y, data=data, kernel='vanilladot')

w = colSums(coef(svm_model) * data[alphaindex(svm_model)[[1]],c('x','y')])  
b = b(svm_model)
data[, 1:2] = sapply(data[,1:2], scale)

ggplot(data,aes(x, y, color=class)) +
  geom_point(size=3) + geom_abline(intercept=b/sqrt(sum(w^2)), slope=sqrt(sum(w^2))) 
```


![](https://latex.codecogs.com/gif.latex?!!!!!!!!!!!!!!!!!!!!!!!!)

## Ядро

Пространство H должно быть наделено скалярным произведением, в частности, подойдёт любое евклидово, а в общем случае и гильбертово, пространство. Это даёт возможность заменить скалярное произведение в пространстве X на ядро — функцию, являющуюся скалярным произведением в некотором H.
## ROC-кривая
Рассмотрим задачу классификации на два класса, ![](https://latex.codecogs.com/gif.latex?Y%3D%5Cleft%20%5C%7B%20-1%3B&plus;1%20%5Cright%20%5C%7D), и модель
алгоритмов ![](https://latex.codecogs.com/gif.latex?a%28x%2C%20w%29%3Dsign%28f%28x%2C%20w%29%20-%20w_0%29), где ![](https://latex.codecogs.com/gif.latex?w_0%20%5Cin%20%5Cmathbb%7BR%7D) — аддитивный параметр дискриминантной функции. В теории нейронных сетей его называют порогом активации.
В случае линейной дискриминантной функции параметр ![](https://latex.codecogs.com/gif.latex?w_0) определяется отношением потерь:

![](https://latex.codecogs.com/gif.latex?w_0%20%3D%5Cln%20%5Cfrac%7B-%5Clambda%20%7D%7B&plus;%5Clambda%20%7D)


где λ+ и λ− — величина потери при ошибке на объекте класса «+1» и «−1» соответственно. На практике отношение потерь может многократно пересматриваться. Поэтому вводится специальная характеристика — ROC-кривая, которая показывает, что происходит с числом ошибок обоих типов, если изменяется отношение потерь.



ROC-кривая - это кривая ошибок. Площадь под ROC-кривой AUC (Area Under Curve) является агрегированной характеристикой качества классификации, не зависящей от соотношения цен ошибок. Чем больше значение AUC, тем «лучше» модель классификации. Также будем использовать FPR, TPR: доля ложных положительных классификаций (False Positive Rate, FPR) и доля верных положительных классификаций (True Positive Rate, TPR).
Алгоритм ROC-кривой с помощью библиотеки kernlab:
1. Вычислим количество представителей каждаго класса ![](https://latex.codecogs.com/gif.latex?m1,m2)
2. Упорядочим выборку по убыванию
3. Установим начальную точку ROC-кривой: ![](https://latex.codecogs.com/gif.latex?(FPR_0,TPR_0)=(0,0)) и ![](https://latex.codecogs.com/gif.latex?AUC=0)
4. если точка принадлежит второму классу, то смещаемся влево
    ![](https://latex.codecogs.com/gif.latex?FPR_i=FPR_{i-1}+\frac{1}{m2};)
    ![](https://latex.codecogs.com/gif.latex?TPR_i=TPR_{i-1};)
    ![](https://latex.codecogs.com/gif.latex?AUC=AUC+\frac{1}{m2}TPR_i)
5. иначе смещаемся вверх
    ![](https://latex.codecogs.com/gif.latex?FPR_i=FPR_{i-1};)
    ![](https://latex.codecogs.com/gif.latex?TPR_i=TPR_{i-1}+\frac{1}{m1};)

```R
FPR<-c()
FPR[1]=0
TPR<-c()
TPR[1]=0
AUC<-0
m1=table(data[3])[[1]]
m2=table(data[3])[[2]]
orderedXl <- data[order(data[1],data[1], decreasing=T), ]

for (i in 2:dim(orderedXl)[1]){
  if (orderedXl[i,3]=="versicolor"){
    FPR[i]=FPR[i-1]+(1/m2)
    TPR[i]=TPR[i-1]
    AUC=AUC+(1/m2)*TPR[i]
  }else{
    FPR[i]=FPR[i-1]
    TPR[i]=TPR[i-1]+(1/m1)
  }
}
```


[](https://latex.codecogs.com/gif.latex?!!!!!!!!!!!!!!!!!!!!!!!)











