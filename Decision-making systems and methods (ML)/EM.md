#  Разделение смеси распределений


В тех случаях, когда «форму» класса не удаётся описать каким-либо одним распределением, можно попробовать описать её смесью распределений.

![equation](CodeCogsEqn.gif) 

где ![](https://latex.codecogs.com/gif.latex?p_j(x)) — функция правдоподобия ![](https://latex.codecogs.com/gif.latex?j)-ой компоненты смеси, ![](https://latex.codecogs.com/gif.latex?w_j) — её априорная вероятность.

>**Априорная  (безусловная) вероятность** представляет собой степень уверенности в том, что данное событие произошло, в отсутствие любой другой информации, связанной с этим событием. Представление в задачах: вероятность принадлежности объекта ![](https://latex.codecogs.com/gif.latex?X) к классу ![](https://latex.codecogs.com/gif.latex?A) без учета его признаков.
>**Значение правдоподобия** - согласование (схожесть) с выборкой.

Иными словами, "выбрать объект x из смеси ![](https://latex.codecogs.com/gif.latex?p(x))" означает сначала выбрать ![](https://latex.codecogs.com/gif.latex?j)-ю компоненту смеси из дискретного распределения ![](https://latex.codecogs.com/gif.latex?\{w_1,&space;\dots&space;,&space;w_k\}), затем выбрать объект ![](https://latex.codecogs.com/gif.latex?x) согласно плотности ![](https://latex.codecogs.com/gif.latex?p_j(x)).
**Задача разделения смеси** - оценить вектор параметров, имея выборку,  смесь, количество распределений и функцию распределения.
## EM-алгоритм (expectation-maximization)
***Почему используем***
Принцип максимума правдоподобия «в лоб», приводит к слишком громоздкой оптимизационной задаче (долго решается).  
***Идея алгоритма***
Искусственно вводится вспомогательный вектор скрытых (hidden) переменных ![](https://latex.codecogs.com/gif.latex?G), обладающий двумя замечательными свойствами. 
EM-алгоритм состоит из итерационного повторения двух шагов. 
На ![](https://latex.codecogs.com/gif.latex?E)-шаге вычисляется ожидаемое значение (expectation) вектора скрытых переменных ![](https://latex.codecogs.com/gif.latex?G) по текущему приближению вектора параметров ![](https://latex.codecogs.com/gif.latex?\Theta). На ![](https://latex.codecogs.com/gif.latex?M)-шаге решается задача максимизации правдоподобия (maximization) и находится следующее приближение вектора параметров по текущим значениям векторов ![](https://latex.codecogs.com/gif.latex?G) и ![](https://latex.codecogs.com/gif.latex?\Theta).
### E-шаг (expectation)
Обозначим через ![](https://latex.codecogs.com/gif.latex?p(x,\theta_j)) плотность вероятности того, что объект ![](https://latex.codecogs.com/gif.latex?x) получен из ![](https://latex.codecogs.com/gif.latex?j)-й компоненты смеси. По формуле условной вероятности

![](https://latex.codecogs.com/gif.latex?p(x,\theta_j)=p(x)P(\theta_j|x)=w_jp_j(x))

Введём обозначение ![](https://latex.codecogs.com/gif.latex?g_{ij}\equivP(\theta_j|x_i)). Это неизвестная апостериорная вероятность того, что обучающий объект ![](https://latex.codecogs.com/gif.latex?x_i) получен из ![](https://latex.codecogs.com/gif.latex?j)-й компоненты смеси. Каждый объект обязательно принадлежит какой-то компоненте, поэтому справедлива формула полной вероятности:
![](https://latex.codecogs.com/gif.latex?\sum_{j=1}^kg_{ij}=1) для всех ![](https://latex.codecogs.com/gif.latex?i=1,\dots,&space;l)
Тогда
![](https://latex.codecogs.com/gif.latex?g_{ij}=\frac{w_jp_j(x_i)}{\sum_{s=1}^k&space;w_sp_s(x_i)})
### M-шаг (maximization)
M-шаг сводится к вычислению весов компонент ![](https://latex.codecogs.com/gif.latex?w_j) как средних арифметических и оцениванию параметров компонент ![](https://latex.codecogs.com/gif.latex?\thete_j) путём решения ![](https://latex.codecogs.com/gif.latex?k) независимых оптимизационных задач.Разделение переменных возможно благодаря удачному введению скрытых переменных.
### Проблема выбора начального приближения. 
Алгоритм EM сходится при достаточно общих предположениях к локальному оптимуму. Качество этого решения и скорость сходимости существенно зависят от начального приближения. Сходимость ухудшается в тех случаях, когда делается попытка поместить несколько компонент в один фактический сгусток распределения, либо разместить компоненту посередине между сгустками. 
*Решение* заключается в том, чтобы выбрать параметры начальных компонент случайным образом. Другая идея — взять в качестве центров компонент ![](https://latex.codecogs.com/gif.latex?k) объектов, максимально удалённых друг от друга. Можно стартовать итерационный процесс много раз из различных начальных приближений и затем выбрать наилучшее решение.
### Последовательное добавление компонент
EM-алгоритм с последовательным добавлением компонент позволяет решить две проблемы сразу — выбора начального приближения и выбора числа компонент.
***Идея*** 
Имея некоторый набор компонент, можно выделить объекты ![](https://latex.codecogs.com/gif.latex?x_i), которые хуже всего описываются смесью — это объекты с наименьшими значениями правдоподобия ![](https://latex.codecogs.com/gif.latex?p(x_i)). По этим объектам строится ещё одна компонента. Затем она добавляется в смесь и запускаются EM-итерации, чтобы новая компонента и старые «притёрлись друг к другу». Так продолжается до тех пор, пока все объекты не окажутся покрыты компонентами.


Существуют и другие разновидности EM-алгоритма, например, *обобщенный* и *стохастический (SEM)*. При обобщенном EM-алгоритме на M-шаге не добиваются высокой точности решения оптимизационных задач, а делается одна или несколько итераций для смещения в направлении максимума, и затем возвращаются к E-шагу. Основное отличие стохастического EM-алгоритма в том, что вместо максимизации взвешенного правдоподобия

![](https://latex.codecogs.com/gif.latex?\theta_j=\arg&space;\max_j\sum_{i=1}^mg_{ij}\ln\o(x_i;\theta))

решается задача максимизации обычного, невзвешенного, правдоподобия

![](https://latex.codecogs.com/gif.latex?\theta_j=\arg&space;\max_j\sum_{x_i&space;\in&space;X_j}\ln\o(x_i;\theta))

где выборки ![](https://latex.codecogs.com/gif.latex?X_j) генерируются из ![](https://latex.codecogs.com/gif.latex?X_m) путем стохастического моделирования.
Преимущества SEM обусловлены тем, что рандомизация «выбивает» оптимизационный процесс из локальных максимумов. SEM работает быстрее обычного детерминированного EM, и его результаты меньше зависят от начального приближения. Как правило, SEM находит экстремум ![](https://latex.codecogs.com/gif.latex?Q(\Theta)), более близкий к глобальному.





