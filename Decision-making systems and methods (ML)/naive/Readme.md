# Наивный нормальный байесовский классификатор
Байесовский классификатор —  классификация, основанная на принципе максимума апостериорной вероятности. 
>Апостериорной называют условную вероятность значения, принимаемого случайной переменной, которое назначается после принятия во внимание некоторой новой, связанной с ней информации, и вычисляется с помощью теоремы Байеса. Иными словами, это вероятность события A при условии, что произошло событие B. **(ее можно определить только после того, как мы узнаем признаки объекта)**
##### Теорема (об оптимальности байесовского классификатора):
Если известны априорные вероятности ![](https://latex.codecogs.com/gif.latex?P_y) и функции правдоподобия ![](https://latex.codecogs.com/gif.latex?p_y(x)), поставим в соответствие величину потери ![](https://latex.codecogs.com/gif.latex?\lambda_{ys}) при отнесении объекта класса ![](https://latex.codecogs.com/gif.latex?y) к классу ![](https://latex.codecogs.com/gif.latex?s), a ![](https://latex.codecogs.com/gif.latex?\lambda_{yy}=0) , то минимум среднего риска ![](https://latex.codecogs.com/gif.latex?R(a)) достигается алгоритмом:


![](https://latex.codecogs.com/gif.latex?a(x)=\arg&space;\max_{y\in&space;Y}\lambda_yP_yp_y(x).)


Оптимальный алгоритм классификации  можно переписать через апостериорные вероятности, поэтому данное выражение называют оптимальным байесовским правилом.

## Наивный байесовский классификатор
Наивный байесовский классификатор снован на формуле, что объекты описываются независимыми признаками (из-за этого название наивный). Предположение о независимости существенно упрощает задачу, так как оценить ![](https://latex.codecogs.com/gif.latex?n) одномерных плотностей гораздо легче, чем одну ![](https://latex.codecogs.com/gif.latex?n)-мерную плотность.

Будем полагать, что все объекты описываются n числовыми признаками. Обозначим через ![](https://latex.codecogs.com/gif.latex?x=(\xi_1,\dots,\xi_n)), где ![](https://latex.codecogs.com/gif.latex?\xi_j=f_j(x)). Если все признаки независимы, то все функции правдоподобия представлены в виде произведения:


![](https://latex.codecogs.com/gif.latex?p_y(x)=p_{y1}(\xi_1),\dots,p_{yn}(\xi_n))

Подставим эмпирические оценки одномерных плотностей в оптимальный байесовский классификатор:

>На практике при умножении очень малых условных вероятностей может наблюдаться потеря значащих разрядов, в связи с чем вместо самих оценок апостериорных вероятностей применяют логарифмы этих вероятностей. Поскольку логарифм - монотонно возрастающая функция, то класс   с наибольшим значением логарифма вероятности останется наиболее вероятным. 


![](https://latex.codecogs.com/gif.latex?a%28x%29%3D%5Carg%20%5Cmax_%7By%5Cin%20Y%7D%28%5Cln%28%5Clambda_yP_y%29&plus;%5Csum_%7Bj%3D1%7D%5En%20%5Cln%28p_%7Byj%7D%28%5Cxi_j%29%29%29.)

## Алгоритм
Используется нормальный байесовский классификатор. Функция правдоподобия расчитывается по формуле:
![](https://latex.codecogs.com/gif.latex?p%28x%29%3D%5Cfrac%7B1%7D%7B%5Csqrt%282%5Cpi%20D%29%7D%20e%5E%7B-%5Cfrac%7B%28x-E%29%5E2%7D%7B2D%7D%7D)
где ![](https://latex.codecogs.com/gif.latex?D) - дисперсия, ![](https://latex.codecogs.com/gif.latex?E) - математическое ожидание
```R
for (j in 1:3){
      ans[j]<-1/sqrt(2*pi*D_new[j])*(exp(-(data_new-E_new[j])^2/(2*D_new[j])))
  }
```
Дисперсия просчитывается по формуле:
![](https://latex.codecogs.com/gif.latex?D%28X%29%3D%5Csum_%7Bi%3D1%7D%5En%20x_i%5E2p_i-%28%5Csum_%7Bi%3D1%7D%5En%20x_ip_i%29%5E2)

```R
 for (i in 1:N){
    a1[i]<-(A[i,2])^2*p
    a2[i]<-(A[i,2]*p)
  }
  ans[2]<-sum(a1)-(sum(a2))^2
```
где ![](https://latex.codecogs.com/gif.latex?p_i) - вероятность появления объекта.
Математическое ожидание просчитываю как среднее арифметическое ( все события равновероятны)
```R
for (i in 1:n){
    ans[i]<-mean(A[,i]) #равномерное распределение
  }
```

За обучающую выборку были взяты Ирисы Фишера. Так как предположили, что все классы равнозначны (основа наивного байесовского классификатора), то величина потери ![](https://latex.codecogs.com/gif.latex?\lambda=1) для всех классов 
Результат работы
![](https://github.com/Elzara20/university/blob/master/pictures/NB1.jpeg)
