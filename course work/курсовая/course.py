import numpy as np
#X-входные параметры, Y-выходной сигнал
X = np.array(([1716,12158100,0.72819,0.00676,4.01111,0.34626], 
			  [1358,121972200,0.79910,0.00779,3.93152,0.78071],
			  [1484,22481790,0.69068,0.00135,4.03929,0.74227],
			  [1674,18781300,0.78829,0.01469,4.16528,0.42182],
			  [1555,11511200,0.97625,0.00336,3.92381,0.12019],
			  [1725,30112300,0.76267,0.01445,4.15556,0.41732]), dtype=float)
Y = np.array(([6.6], [6.4], [5.1], [4.7], [5.0], [4.8]), dtype=float) #перезаполнение, поэтому dtype=np.float128
#скаляризация параметров, так как числовые значения очень разняться 
X = X/np.amax(X, axis=0) #находим максимум массива по столбцам и делим входные значения на полученный максимум
Y = Y/9.5 #максимальная магнитуда = 10

class NN():
	# конструктор
	def __init__(self):
		# нейросеть состоит из входного слоя (6 нейронов, входных значений)
		# скрытого слоя (5 нейронов)
		# выходного слоя (1 нейрон, полученный результат)
		# между 3 слоями создам 2 матрицы весов: от входа до скрытого слоя (w_ih)
		# от скрытого слоя до выходного значения (w_ho)
		self.w_ih = np.random.randn(6, 5)
		self.w_ho = np.random.randn(5, 1)
	# активационная функция, сигмоида
	def sig(self,x):
		return 1/(1+np.exp(-x))
	# производная активационной функции
	def deriv_sig(self,x):
		return x*(1-x)
	# feedforward, прямое распространение
	def feedforward(self,X):
		# np.dot(X,self.w_ih) - перемножаю матрицу входных значений и матрицу весов
		# self.sig(np.dot(X,self.w_ih)) - для полученного результата применяю активационную функцию
		# далее произвожу аналогичные действия для второй матрицы весов  (w_ho)
		self.one=self.sig(np.dot(X,self.w_ih))
		self.two=self.sig(np.dot(self.one,self.w_ho))
		return self.two
	# обратное распространение ошибки на основе приминения метода градиентного спуска
	def backprop(self,X,Y,pred):
		# (y-pred) - разница между предсказанным значением и реальным, полученная ошибка
		# (y-pred)*self.deriv_sig(pred) - производная сигмоиды, умноженная на полученную ошибку
		#  так поступаем, так как хотим понять как веса влияют на данные и определить скорость изменения подсчитанных данных
		self.a1=(Y-pred)*self.deriv_sig(pred)
		#  хотим узнать как скрытый слой повлиял на полученную ошибку
		self.hidlay1=self.a1.dot(self.w_ho.T)
		self.hidlay2=self.hidlay1*self.deriv_sig(self.one)
		#  изменяем веса 
		self.w_ih+=X.T.dot(self.hidlay2)
		self.w_ho+=self.one.T.dot(self.a1)


nn=NN()
for i in range(1000):
	#print "Input: \n" + str(X)
	print ("Output:")
	print(Y)
	print ("Predicted:")
	print(nn.feedforward(X))
	print ("MSE:")
	print(0.5*(np.square(Y - nn.feedforward(X))))

	pred=nn.feedforward(X)
	nn.backprop(X,Y,pred)


