# Метрические алгоритмы классификации
***Метрические методы классификации*** - это методы обучения, основанные на исследовании сходства объектов.
К метрическим методам относятся:
- алгоритм ближайших соседей
- метод парзеновского окна
- метод потенциальных функций
- отбор эталонных объектов
## Алгоритм ближайших соседей
Данный алгоритм классифицирует объект к тому классу, которому принадлежат его соседи.
В дальнейшем, при рассмотрении методов алгоритма, обучающей выборкой будет набор данных - Ирисы Фишера (длина и ширина лепестка), а принадлежность к классу определять по виду ириса. Для подсчета расстояния использую расстояние Евклида.
### Метод первого ближайшего соседа
Модель алгоритма:
1. Дана обучающая выборка xl и случайно выбранная точка z (z нужно классифицировать)
2. Вычисляем расстояние от z до каждого объекта из выбоки xl
3. Сортируем расстояния от минимума к максимуму
4. Определяем к какому классу относиться объект по первому соседу
 ```R
OneNN <- function(xl, z){
    l <- dim(xl)[1] # строки
    n <- dim(xl)[2] - 1 #столбцы
    dist <-c(0) #расстояние
    # вычисление расстояния
    for (i in 1:l){
        dist[i] <- c(Euclid(xl[i, 1:n], z))
    }
    # сортировка выборки по расстоянию
    OrderedXl <- xl[order(dist), ]
    # определяем к какому классу принадлежит самый первый сосед
    class <- OrderedXl[1,n+1]  
    return (class)
    }
```

Пример работы алгоритма показана на рисунках
![][image1]
![][image2]
![][image3]

[image1]: //1OneNN.jpg/250x100
[image2]: //2OneNN.jpg/200x100
[image3]: //3OneNN.jpg/150x100

### Метод k ближайших соседей
Модель алгоритма:
1. Дана обучающая выборка xl, случайно выбранная точка z, количество соседей k (k выбираю сама)
2. Вычисляем расстояние от z до каждого объекта из выбоки xl
3. Сортируем расстояния от минимума к максимуму
4. Определяем какой класс среди k ближайших соседей часто встречается
```R
KNN <- function(xl, z, k){
    l <- dim(xl)[1]  # строки
    n <- dim(xl)[2] - 1 #  столбцы 
    dist <-c(0)

    for (i in 1:l){
        dist[i] <- c(Euclid(xl[i, 1:n], z))
    }
    #сортировка выборки по расстоянию
    orderedXl <- xl[order(dist), ]
    #частота появления определенного цвета
    counts <- table(orderedXl[1:k, n +1])
    # определение наиболее часто появляющегося класса
    class <- which.max(counts)
    # возвращаем цвет наиболее близкого класса
    return (names(class))
}
```
Однако не всегда случайно подобранное k является правильным параметром. Критерий скользящего контроля с исключением объектов по одному (LOO, leave-one-out) позволяет определить оптимальное значение k. 
Принцип работы критерия: 
1. "Прячем" объект из выборки
2. С помощью метода ближайших соседей (по порядку определяя k) определяем к какому классу принадлежит объект
3. Сравниваем полученные значения
    3.1 Если полученный класс совпадает с действительным классом объекта, то ошибка равна 0
    3.2 Если полученный класс не совпадает с действительным классом объекта, то ошибка равна 1
4. Суммируем полученные ошибки и определяем вероятность появления ошибки при каждом k
5. Ответом будет k с наименьшим значением вероятности 
 ```R
 LOO <- function(a){
    len <- dim(a)[1]
    ans <- c()
    sum <- 0
     for (j in 1:len){ # paccматриваем всех соседей
        sum <- 0
        for (i in 1:len){  # рассматриваем данные из iris 
            z <- a[i, 1:2] # координаты
            cl <- KNN(a[-i,], z, j) # применяем метод ближайших соседей
            # если классы не совпадают, то прибавляем  1      
            if (a[i, 3] != cl){                
                sum <- sum + 1
            }    
        }       
        ans <- c(ans, sum/len)  #записываем вероятность появления ошибки при каждом k (число соседей)    
    }
    plot(1:len,ans, type="l") #график
    kj <- 0 # поиск k с минимальной вероятностью
     for (j in 1:10){
        if (ans[j]==min(ans)){
            kj <- j
        }
    }
    return(kj)
    
}
```
Для Ирисов Фишера подходящим k является 6.

Однако недостатком метода ближайших k соседей является то, что правильная классификация объекта (распознавание) может достигаться на нескольких классах. Например, при четном числе соседей, наиболее встречающихся классов может быть поровну. Более подходящий вариант для многих классах - это ввести строго убывающую последовательность вещественных весов.
### Метод k взвешенных ближайших соседей
Модель алгоритма:
1. Дана обучающая выборка xl, случайно выбранная точка z, количество соседей k (k=6), параметр для нахождения весов q.
2. Вычисляем расстояние от z до каждого объекта из выбоки xl
3. Сортируем расстояния от минимума к максимуму
4. Определяем какой класс встречается чаще среди k ближайших соседей 
    4.1 Просчитываем взвешенную сумму по q^i
    4.1 Определяем максимальное значение

q принадлежит промежутку [0.0067,1] и можно также подобрать по LOO.
```R
KwNN <- function(xl,z,k,q){
    l <- dim(xl)[1] # строки
    n <- dim(xl)[2]-1 # столбцы 
    dist <-c(0)
    Colors <- c("setosa" = 0, "versicolor" = 0, "virginica" = 0)
    for (i in 1:l){
        dist[i] <- c(Euclid(xl[i, 1:n], z[,1:n]))
    }
    #сортировка выборки по расстоянию
    OrderedXl <- xl[order(dist), ]
    for (c in names(Colors)){ # имена классов
        for (i in 1:k){     # соседи       
            if(OrderedXl[i,3]==c){
                Colors[c] <- Colors[c]+q^i # взвешенная сумма для k ближайших соседей
            }           
        }     
    }
    return(names(which.max(Colors)))
}
```
Для Ирисов Фишера подходящим q является .
