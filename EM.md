#  Разделение смеси распределений


В тех случаях, когда «форму» класса не удаётся описать каким-либо одним распределением, можно попробовать описать её смесью распределений.

![equation](CodeCogsEqn.gif)

где ![equation](CodeCogsEqn (1).gif) — функция правдоподобия ![equation](CodeCogsEqn (2).gif)-ой компоненты смеси, ![equation](CodeCogsEqn (3).gif) — её априорная вероятность.

>**Априорная  (безусловная) вероятность** представляет собой степень уверенности в том, что данное событие произошло, в отсутствие любой другой информации, связанной с этим событием. Представление в задачах: вероятность принадлежности объекта ![](https://latex.codecogs.com/gif.latex?X) к классу ![](https://latex.codecogs.com/gif.latex?A) без учета его признаков.
>**Значение правдоподобия** - согласование (схожесть) с выборкой.

Иными словами, "выбрать объект x из смеси ![equation](CodeCogsEqn (4).gif)" означает сначала выбрать ![](CodeCogsEqn (2).gif)-ю компоненту смеси из дискретного распределения ${w_1, . . . , w_k}$, затем выбрать объект $x$ согласно плотности![](CodeCogsEqn (1).gif).
**Задача разделения смеси** - оценить вектор параметров, имея выборку,  смесь, количество распределений и функцию распределения.
## EM-алгоритм (expectation-maximization)
***Почему используем***
Принцип максимума правдоподобия «в лоб», приводит к слишком громоздкой оптимизационной задаче (долго решается).  
***Идея алгоритма***
Искусственно вводится вспомогательный вектор скрытых (hidden) переменных $G$, обладающий двумя замечательными свойствами. 
EM-алгоритм состоит из итерационного повторения двух шагов. 
На $E$-шаге вычисляется ожидаемое значение (expectation) вектора скрытых переменных $G$ по текущему приближению вектора параметров. На $М$-шаге решается задача максимизации правдоподобия (maximization) и находится следующее приближение вектора параметров по текущим значениям векторов $G$ и вектора параметров.

