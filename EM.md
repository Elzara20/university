<!-- center code math uml theme:white -->
#  Разделение смеси распределений


В тех случаях, когда «форму» класса не удаётся описать каким-либо одним распределением, можно попробовать описать её смесью распределений.

\(p(x)=\sum_{j=1}^{k}w_jp_j(x), \sum_{j=1}^{k}w_j=1, w_j\leqslant0\)
где $p_j (x)$ — функция правдоподобия $j$-ой компоненты смеси, $w_j$ — её априорная вероятность.

>**Априорная  (безусловная) вероятность** представляет собой степень уверенности в том, что данное событие произошло, в отсутствие любой другой информации, связанной с этим событием. Представление в задачах: вероятность принадлежности объекта $X$ к классу $A$ без учета его признаков.
>**Значение правдоподобия** - согласование (схожесть) с выборкой.

Иными словами, "выбрать объект x из смеси $p(x)$" означает сначала выбрать $j$-ю компоненту смеси из дискретного распределения ${w_1, . . . , w_k}$, затем выбрать объект $x$ согласно плотности $p_j(x)$.
**Задача разделения смеси** - оценить вектор параметров, имея выборку,  смесь, количество распределений и функцию распределения.
## EM-алгоритм (expectation-maximization)
***Почему используем***
Принцип максимума правдоподобия «в лоб», приводит к слишком громоздкой оптимизационной задаче (долго решается).  
***Идея алгоритма***
Искусственно вводится вспомогательный вектор скрытых (hidden) переменных $G$, обладающий двумя замечательными свойствами. 
EM-алгоритм состоит из итерационного повторения двух шагов. 
На $E$-шаге вычисляется ожидаемое значение (expectation) вектора скрытых переменных $G$ по текущему приближению вектора параметров. На $М$-шаге решается задача максимизации правдоподобия (maximization) и находится следующее приближение вектора параметров по текущим значениям векторов $G$ и вектора параметров.

